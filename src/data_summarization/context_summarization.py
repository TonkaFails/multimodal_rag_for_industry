import pandas as pd
import torch
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain.schema.runnable import RunnableLambda
from transformers import pipeline, LlavaNextForConditionalGeneration, LlavaNextProcessor
from tqdm.auto import tqdm
from typing import List, Tuple
import sys
import os
import io
import gc  # For manual garbage collection
from PIL import Image  # Ensure PIL is imported

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from utils.base64_utils.base64_utils import *
from utils.model_loading_and_prompting.llava import format_prompt_with_image, llava_call
from rag_env import INPUT_DATA, IMG_SUMMARIES_CACHE_DIR, TEXT_SUMMARIES_CACHE_DIR


class TextSummarizer:
    """  
    A class to summarize texts using either AzureOpenAI's model or the LLama 3 8B model.
  
    Attributes:
        model_type (str): Type of the model to use for summarization.
        cache_path (str): Path to the directory where summaries will be cached as a CSV file.
        model (AzureChatOpenAI or HuggingFacePipeline): The summarization model loaded based on `model_type`.
        cache_file (str): The complete path to the cached CSV file containing text summaries.
        df (pd.DataFrame): DataFrame to store and manage texts and their corresponding summaries.
    """    
    
    def __init__(self, model_type: str, cache_path: str):
        """  
        Initializes the TextSummarizer object.
  
        :param model_type: The type of model to be used for summarization.
        :param cache_path: The directory path where the summaries will be cached.
        """ 
        self.model_type = model_type
        #config = get_azure_config()
        
        
        pipe = pipeline("text-generation",
                        model="meta-llama/Meta-Llama-3-8B-Instruct",
                        model_kwargs={"torch_dtype": torch.bfloat16},
                        device_map="auto")

        self.model = HuggingFacePipeline(pipeline=pipe)
            
        # Load cached DataFrame if it exists
        self.cache_file = os.path.join(cache_path, f'text_summaries_{self.model_type}.csv')
        if os.path.exists(self.cache_file):
            self.df = pd.read_csv(self.cache_file)
        else:
            # Initialize DataFrame if it doesn't exist
            self.df = pd.DataFrame(columns=['text', 'text_summary'])
            self.df.to_csv(self.cache_file, index=False)
    
    
    def summarize(self, texts: List[str]) -> List[str]:
        """  
        Generates summaries for a list of texts.
        This method determines which model to use based on the `model_type` attribute,
        and then calls the appropriate summarization method.
  
        :param texts: A list of texts to be summarized.
        :return: A list of summarized texts.
        """  
        
        text_summaries = self.summarize_llama(texts)
        return text_summaries
    
    
    def summarize_llama(self, texts: List[str]) -> List[str]:
        """  
        Summarizes texts using the LLama 3 8B model.
        Iterates over the list of texts, checks if a summary already exists in the cache,
        and if not, generates a new summary using the LLama 3 8B model. Updates the cache
        with new summaries.
  
        :param texts: A list of texts to be summarized.
        :return: A list of summaries generated by the LLama model.
        """
  
        # Iterate over texts and generate summaries with a progress bar
        with tqdm(total=len(texts), desc="Summarizing texts") as pbar:
            for i, text in enumerate(texts):
                self.df.at[i, 'text'] = text

                pbar.update(1)
  
                # Skip if summary already exists
                if i < len(self.df) and pd.notna(self.df.at[i, 'text_summary']):
                    print(f"Summary for text {i + 1} already exists. Skipping...")
                    continue
                
                template = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_msg}
                        <|start_header_id|>user<|end_header_id|>Text: {text}\nSummary:\n<|eot_id|>
                        <|start_header_id|>assistant<|end_header_id|>"""

                system = """You are an assistant tasked with summarizing text for retrieval.
                    These summaries will be embedded and used to retrieve the raw text elements.
                    Give a concise summary of the text that is well optimized for retrieval.
                    Only output the summary, no additional explanation.\n"""

                prompt = PromptTemplate.from_template(template)
                prompt = prompt.partial(system_msg=system)

                print("Summarizing texts...")
                
                # Remove the text given to the model as input from the generation
                strip_output = RunnableLambda(lambda output: output.split("<|start_header_id|>assistant<|end_header_id|>",1)[1].strip())

                # Text summary chain
                summarize_chain = {"text": lambda text: text} | prompt | self.model | strip_output | StrOutputParser()
                summary = summarize_chain.invoke({"text": text})

                # Update DataFrame with new summary
                self.df.at[i, 'text_summary'] = summary
                # Cache the DataFrame after each generation
                self.df.to_csv(self.cache_file, index=False)
  
        return self.df['text_summary'].tolist()


class ImageSummarizer:
    """
    A class to summarize images using different models. It can encode images in base64,
    generate textual summaries, and cache these summaries for quick retrieval.
    """
    def __init__(self, model, tokenizer=None):
        """
        Initializes the ImageSummarizer with a specific model and an optional tokenizer.
          
        :param model: The model to be used for generating image summaries. This can be an instance of either
                      AzureChatOpenAI, LlavaNextForConditionalGeneration, or any model that supports image summarization.
        :param tokenizer: The tokenizer to be used with the model, if necessary. This is model-dependent and optional.
        """
        self.model = model
        self.tokenizer = tokenizer
        
        
    def summarize(self, image_bytes_list: List[bytes], cache_path: str) -> Tuple[List[str], List[str]]:
        """  
        Generate summaries and base64 encoded strings for images. This function also checks for cached summaries
        to avoid re-processing images. If a summary does not exist, it will generate a new one, update the cache,
        and return the summaries along with their base64 encoded strings.
          
        :param image_bytes_list: A list of image bytes. Each entry in the list should be the binary content of an image file.
        :param cache_path: The file system path where cached summaries are stored. This path is used to store summaries
                           in a CSV file to avoid re-processing images.
        :return: A tuple containing two lists - the first list contains the base64 encoded strings of the images,
                 and the second list contains the textual summaries of the images.
        """
        # Initialize base64 list
        img_base64_list = []

        model_type = "llava"
        # Load cached DataFrame if it exists
        cache_file = os.path.join(cache_path, f'image_summaries_{model_type}.csv')
        if os.path.exists(cache_file):
            df = pd.read_csv(cache_file)
        else:
            # Initialize DataFrame if it doesn't exist
            df = pd.DataFrame(columns=['image_summary'])
            df.to_csv(cache_file, index=False)

        # Prompt template
        prompt = """You are an assistant tasked with summarizing images for retrieval. \
                    These summaries will be embedded and used to retrieve the raw image. \
                    Give a concise summary of the image that is well optimized for retrieval."""

        # Iterate over image bytes and generate base64 encoded string
        for i, image_bytes in enumerate(image_bytes_list):
            # Convert image bytes to base64
            try:
                img_base64 = encode_image_from_bytes(image_bytes)
                img_base64_list.append(img_base64)
            except Exception as e:
                print(f"Failed to encode img {i}: {e}")
                continue

            # Skip if summary already exists
            if i < len(df) and pd.notna(df.at[i, 'image_summary']):
                print(f"Summary for image {i + 1} already exists. Skipping...")
                continue

            try:
                print(f"Summarizing image {i + 1} of {len(image_bytes_list)}")
                summary_content = self.summarize_image_llava(img_base64, prompt)
            except Exception as e:
                print(f"Failed to summarize img {i}: {e}")
                with open('summarization_fails.txt', 'a') as f:
                    f.write(f"Failed to summarize img {i}\n")
                continue

            # Update DataFrame with new summary
            df.at[i, 'image_summary'] = summary_content

            # Cache the DataFrame after each generation
            df.to_csv(cache_file, index=False)

            # Run garbage collection to free memory after processing each image
            gc.collect()

        return img_base64_list, df['image_summary'].tolist()
        
        
    def summarize_image_llava(self, img_base64: str, prompt: str) -> str:
        """
        Summarizes a single image using the Llava model. This method now opens the image in a context
        manager and wraps the model call in torch.inference_mode() to reduce memory overhead.
        """
        llava_prompt = format_prompt_with_image(prompt)
        image_bytes = decode_image_to_bytes(img_base64)
        # Open the image using a context manager to ensure it is closed promptly
        with Image.open(io.BytesIO(image_bytes)) as image:
            # Use inference mode to avoid building gradients and reduce memory usage
            with torch.inference_mode():
                img_summary = llava_call(llava_prompt, self.model, self.tokenizer, device="mps", image=image)
        return img_summary


if __name__ == "__main__":

    # text summarization (unchanged)
    # text_summarizer = TextSummarizer(model_type='llama3', cache_path=TEXT_SUMMARIES_CACHE_DIR)
    # df = pd.read_parquet(INPUT_DATA)
    # texts = list(df.drop_duplicates(subset='text')['text'])
    # text_summarizer.summarize(texts)

    # image summarization
    df = pd.read_parquet(INPUT_DATA)
    filtered_df = df[df['has_image'] == True]
    images = list(filtered_df["image_bytes"])
    
    model_id = "llava-hf/llava-v1.6-mistral-7b-hf"
    processor = LlavaNextProcessor.from_pretrained(model_id)
    # Load the model with low CPU memory usage and half precision to reduce memory pressure
    model = LlavaNextForConditionalGeneration.from_pretrained(
        model_id,
        device_map="mps",
        low_cpu_mem_usage=True,
        torch_dtype=torch.float16
    )
    
    image_summarizer = ImageSummarizer(model, processor)
    image_summarizer.summarize(images, cache_path=IMG_SUMMARIES_CACHE_DIR)
